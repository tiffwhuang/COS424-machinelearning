{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twhuang_asursuck_hw3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuqSvDGlN6x6"
      },
      "source": [
        "NOTE: This code is of significant length and will take several hours to fully run on a regular computer. We suggest running the \"Import & Install Necessary Packages\" and \"Data Preprocessing & New DataSet Creation\" sections prior to deciding what types of analyses should be run.\n",
        "\n",
        "We perform different types of analysis with several subsection for each:\n",
        "1. NetworkX Graph Analysis\n",
        "2. Prediction of wages and instigator status\n",
        "3. PCA & MCA Clustering\n",
        "4. Additional Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXEk5s_-reGJ"
      },
      "source": [
        "# Import & Install Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrwvHdmGGKls"
      },
      "source": [
        "!pip install graspologic # for graph analysis https://graspologic.readthedocs.io/en/latest/\n",
        "!pip install mca # PCA for categorical variables https://pypi.org/project/mca/ \n",
        "# !pip install graph_tool # for graph analysis https://graph-tool.skewed.de/ \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import math\n",
        "import pymc3 as pymc\n",
        "import networkx as nx\n",
        "import graspologic\n",
        "import mca\n",
        "import sklearn\n",
        "import sklearn.metrics as m\n",
        "import sklearn.model_selection as ms\n",
        "# import graph_tool\n",
        "%matplotlib inline\n",
        "\n",
        "# Import DataProcessing & Analysis Packages\n",
        "from scipy.sparse import csr_matrix\n",
        "from pandas import DataFrame\n",
        "from collections import defaultdict\n",
        "from graspologic.plot import heatmap\n",
        "\n",
        "\n",
        "# Import Sklearn packages\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
        "from sklearn.linear_model import LinearRegression, LassoCV, LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import TruncatedSVD, NMF\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.naive_bayes import BernoulliNB,  MultinomialNB\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Import Keras packages for Neural Network \n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Embedding, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, Input, Dropout, GRU, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM\n",
        "from keras import backend as K\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping, CSVLogger, History"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF67dcHylmxw"
      },
      "source": [
        "# Data Preprocessing & New DataSet Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQibYDbishfL"
      },
      "source": [
        "## Clean Officer Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdLB6PWKs0MX"
      },
      "source": [
        "# Quantify Officer Ranks based on NYPD rank strucutre\n",
        "# Note: second set is a repeated with space at end to account for weird data\n",
        "#       set structuring\n",
        "rank_dict = {\n",
        "  'Police Officer': 1, \n",
        "  'Detective Third Grade': 2,\n",
        "  'Detective Second Grade': 3, \n",
        "  'Detective First Grade': 4,\n",
        "  'Sergeant': 4, \n",
        "  'Detective Specialist': 4, \n",
        "  'Lieutenant': 5,\n",
        "  'Captain': 6,\n",
        "  'Deputy Inspector': 7,  \n",
        "  'Inspector': 8, \n",
        "  'Deputy Chief Inspector': 9, \n",
        "  'Assistant Chief Inspector': 10,\n",
        "  'Chief': 11, \n",
        "  'Deputy Commissioner': 12, \n",
        "  'Assistant Deputy Commissioner': 13,\n",
        "  'Assistant Commissioner': 14,\n",
        "  'Commissioner': 15,\n",
        "    \n",
        "  'Police Officer ': 1, \n",
        "  'Detective Third Grade ': 2,\n",
        "  'Detective Second Grade ': 3, \n",
        "  'Detective First Grade ': 4,\n",
        "  'Sergeant ': 4, \n",
        "  'Detective Specialist ': 4, \n",
        "  'Lieutenant ': 5,\n",
        "  'Captain ': 6,\n",
        "  'Deputy Inspector ': 7,  \n",
        "  'Inspector ': 8, \n",
        "  'Deputy Chief Inspector ': 9, \n",
        "  'Assistant Chief Inspector ': 10,\n",
        "  'Chief ': 11, \n",
        "  'Deputy Commissioner ': 12, \n",
        "  'Assistant Deputy Commissioner ': 13,\n",
        "  'Assistant Commissioner ': 14,\n",
        "  'Commissioner ': 15,\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFfbsL_llD0m"
      },
      "source": [
        "file2 = r\"capstat_expanded_dataset.csv\"\n",
        "officers = pd.read_csv(file2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxOJEahI8LPm"
      },
      "source": [
        "# Clean Names for Officers\n",
        "# Truncate names (to match naming convention of CCRB data set)\n",
        "\n",
        "shortF = [] # Shortened first name\n",
        "shortL = [] # Shortened last name\n",
        "shortAll = [] # Shortened full name\n",
        "\n",
        "for name in officers['First Name']:\n",
        "  # Truncate first names of over 10 characters\n",
        "    if len(name) > 10: shortF.append(name[0 : 10])\n",
        "    else: shortF.append(name)\n",
        "        \n",
        "for name in officers['Last Name']:\n",
        "    # Truncate last names of over 15 characters\n",
        "    if len(name) > 15: shortL.append(name[0 : 10])\n",
        "    else: shortL.append(name)\n",
        "        \n",
        "for i in range(0,len(shortF)):\n",
        "    shortAll.append(shortF[i] + ' ' + shortL[i])\n",
        "    \n",
        "# Substitute names in Officer data set\n",
        "officers[\"Full\"] = shortAll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPeb04UfsbeL"
      },
      "source": [
        "## Clean Complaint Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V29LxcQ4P1SS"
      },
      "source": [
        "file1 = r\"CCRB_database_raw.csv\"\n",
        "df = pd.read_csv(file1) # Complaint data set\n",
        "df = df.drop(['ShieldNo', 'AsOfDate'], axis=1)\n",
        "\n",
        "# Fix up some common typos in the dataset. (Given in starter notebook)\n",
        "allegations_correction_dict = {\n",
        "    'Vehicle Searched': 'Vehicle search',\n",
        "    'Other - Ethnic Slur': 'Ethnic Slur',\n",
        "    'Refusal to provide shield number': 'Refusal to provide name/shield number',\n",
        "    'Refusal to provide name': 'Refusal to provide name/shield number',\n",
        "    'Property Damaged': 'Property damaged',\n",
        "    'Gun Pointed': 'Gun pointed/gun drawn',\n",
        "    'Gun pointed': 'Gun pointed/gun drawn',\n",
        "    'Threat of Arrest': 'Threat of arrest',\n",
        "    'Other- Discourtesy': 'Discourtesy',\n",
        "    'Premise Searched': 'Premises entered and/or searched',\n",
        "    'Entry of Premises': 'Premises entered and/or searched',\n",
        "    'Search of Premises': 'Premises entered and/or searched',\n",
        "    'Threat of Summons': 'Threat of summons',\n",
        "    'Property Damaged': 'Property damaged',\n",
        "    'Flashlight As Club': 'Flashlight As club',\n",
        "    'Radio As Club': 'Radio as club',\n",
        "    'Gun Fired': 'Gun fired',\n",
        "    'Gun As Club': 'Gun As club',\n",
        "}\n",
        "\n",
        "penalty_correction_dict = {\n",
        "    'Terminated': 'Termination',\n",
        "}\n",
        "\n",
        "# Additional Replacements (included in starter notebook)\n",
        "df['Allegation'] = df['Allegation'].replace(allegations_correction_dict)\n",
        "df['PenaltyDesc'] = df['PenaltyDesc'].replace(penalty_correction_dict)\n",
        "df['Full Name'] = df['First Name'] + \" \" + df['Last Name']\n",
        "df['Incident Date'] = pd.to_datetime(df['Incident Date'], format='%m/%d/%Y')\n",
        "dates = df[\"Incident Date\"]\n",
        "\n",
        "df['day'] = df['Incident Date'].dt.day\n",
        "df['month'] = df['Incident Date'].dt.month\n",
        "df['year'] = df['Incident Date'].dt.year\n",
        "\n",
        "df = df.drop(['Incident Date'], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HfztxbUsmyA"
      },
      "source": [
        "## Create & Export New Data Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh8QllIT7ySH"
      },
      "source": [
        "### Column names (sparse matrix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo_u2aA01zjE"
      },
      "source": [
        "# All Unique Allegation Types - to create spare matrix\n",
        "allegations = ['2Force', 'Beat', 'Vehicle search', 'Nasty Words', 'Detention',\n",
        "       'Ethnic Slur', 'Black', 'Punch/Kick', 'Dragged/Pulled', 'Animal', 'Refusal to provide name/shield number', 'Word', 'Question',\n",
        "       'Physical force', 'Hit against inanimate object',\n",
        "       'Property damaged', 'Threat of force (verbal or physical)', 'Stop',\n",
        "       'Frisk', 'Gun pointed/gun drawn', 'Threat of arrest',\n",
        "       'Threat to notify ACS', 'Nightstick as club (incl asp & baton)',\n",
        "       'Question and/or stop', 'Other - Force', 'Curse', 'Push/Shove',\n",
        "       'Other - Abuse', 'Threat of force', '2Discourtesy',\n",
        "       'Premises entered and/or searched', 'Forcible Removal to Hospital',\n",
        "       'Refusal to process civilian complaint', 'Slap', 'Race',\n",
        "       'Threat of summons', 'Vehicle stop', 'Search (of person)', 'Mace',\n",
        "       'Retaliatory arrest', 'Vehicle', 'Retaliatory summons',\n",
        "       'Threat to damage/seize property', 'Ethnicity',\n",
        "       '2Abuse of Authority', 'Demeanor/tone', 'Other',\n",
        "       'Nonlethal restraining device', 'Person Searched',\n",
        "       'Nightstick/Billy/Club', 'Other blunt instrument as a club',\n",
        "       'Strip-searched', 'Pepper spray', 'Interference with recording',\n",
        "       'Failure to provide RTKA card', 'Sexual orientation',\n",
        "       'Frisk and/or search', 'Chokehold', 'Gun Drawn',\n",
        "       'Handcuffs too tight', 'Refusal to obtain medical treatment',\n",
        "       'Flashlight As club', 'Threat to Property', 'Seizure of property',\n",
        "       'Arrest/D. A. T.', 'Physical disability', 'Sexist Remark',\n",
        "       'Radio as club', 'Police shield', 'Hispanic', 'Oriental',\n",
        "       'Gun fired', 'Religion', 'Action', 'Gender', 'Sh Refuse Cmp',\n",
        "       'Rude Gesture', 'Jewish', 'Gesture',\n",
        "       'Refusal to show search warrant', 'Gun As club',\n",
        "       'Flashlight as club', 'Gun as club', 'Profane Gesture',\n",
        "       'Restricted Breathing', 'Threat re: removal to hospital',\n",
        "       'Electronic device information deletion',\n",
        "       'Search of recording device',\n",
        "       'Sex Miscon (Sexual/Romantic Proposition)', 'Property Seized',\n",
        "       'Photography/Videography', 'White', 'Arrest/Onlooker',\n",
        "       'Other Asian', 'Gay/Lesbian Slur',\n",
        "       'Improper dissemination of medical info',\n",
        "       'Sex Miscon (Sexual Harassment, Gesture)',\n",
        "       'Sex Miscon (Sexual Harassment, Verbal)',\n",
        "       'Sexual Misconduct (Sexual Humiliation)',\n",
        "       'Refusal to show arrest warrant', 'Threat re: immigration status',\n",
        "       'Body Cavity Searches', 'Failed to Obtain Language Interpretation',\n",
        "       'Gender Identity', 'Questioned immigration status',\n",
        "       'Sex Miscon (Sexually Motivated Frisk)',\n",
        "       'Sex Miscon (Sexually Motiv Strip-Search)']\n",
        "\n",
        "# The Four FADO Allegation Types\n",
        "fadoAllegations = ['Force', 'Abuse of Authority', 'Discourtesy', 'Offensive Language']\n",
        "\n",
        "# All unique litigation results\n",
        "results = ['Unsubstantiated', 'Complainant Unavailable', 'Unfounded',\n",
        "       'Exonerated', 'Alleged Victim Unavailable',\n",
        "       'Complainant Uncooperative',\n",
        "       'Substantiated (Command Lvl Instructions)',\n",
        "       'Substantiated (Formalized Training)', 'Complaint Withdrawn',\n",
        "       'Miscellaneous - Subject Retired', 'Alleged Victim Uncooperative',\n",
        "       'Substantiated (Charges)', 'Substantiated (Command Discipline)',\n",
        "       'Victim Unidentified', 'Substantiated (Command Discipline B)',\n",
        "       'Substantiated (Command Discipline A)', 'Miscellaneous',\n",
        "       'Substantiated (No Recommendations)',\n",
        "       'Closed - Pending Litigation', 'Substantiated (Instructions)',\n",
        "       'Miscellaneous - Subject Resigned',\n",
        "       'Miscellaneous - Subject Terminated', 'Officer(s) Unidentified',\n",
        "       'Witness Uncooperative', 'Substantiated (MOS Unidentified)',\n",
        "       'Witness Unavailable']\n",
        "\n",
        "# The years our data set should cover\n",
        "# NOTE: Can be adapted to any set of years for further analysis\n",
        "# Reasoning for choosing 2000 - 2019 mentioned in the paper\n",
        "years = ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010','2011','2012', '2013', '2014', '2015', '2016', '2017', '2018']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL_gG5XTlD0u"
      },
      "source": [
        "# Create copy of complaints & keep complaints betweeen 2000 and 2018 only (reasoning in paper)\n",
        "copyDf = df.copy()\n",
        "copyDf = copyDf[copyDf.year >= 2000]\n",
        "copyDf = copyDf[copyDf.year <= 2018]\n",
        "copyDf =   copyDf.loc[:, (copyDf != copyDf.iloc[0]).any()]  # remove 0 variance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Zpnlpv7-Wo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxNOq97a79aT"
      },
      "source": [
        "### Extract officer information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "EQpwSGRPlD0v"
      },
      "source": [
        "# Create NEW & improved DataFrame\n",
        "newDf = pd.DataFrame(columns = ['Name', 'Rank', 'County', 'StartYear','rk1', 'rk1yr', 'rk2', 'rk2yr', 'rk3', 'rk3yr', 'wg18', 'wg17', 'wg16', 'wg15', 'wg14', 'wg13', 'wg12', 'wg11', 'wg10', 'wg09', 'wg08', 'wg07', 'wg12', \n",
        "                                   'wg06', 'wg05', 'wg04', 'wg03', 'wg02', 'wg01', 'wg00'])\n",
        "\n",
        "\n",
        "# CLEAN OFFICERS DATABASE\n",
        "# seperate wages, promotions into sparse matrix \n",
        "\n",
        "arr3 = []\n",
        "\n",
        "# iterrate through officer data set and extract name\n",
        "for row in officers.iterrows():\n",
        "    if type(row[1][10]) == float:\n",
        "        continue # if nan -> skip\n",
        "   \n",
        "   # extract name, district, current rank, and start year\n",
        "    arr2 = []\n",
        "    arr2.append(row[1][13])  \n",
        "    arr2.append(row[1][4])\n",
        "    arr2.append(row[1][7])\n",
        "    arr2.append(re.search(r\", (\\d{4})\", row[1][8]).group(1)) # 4 digit start year\n",
        "\n",
        "    # Extract rank history (format e.g.: [FY2018][Detective] [FY2002][Officer])\n",
        "    strRank = row[1][9] \n",
        "\n",
        "    # iterate through splits to extract data into array form\n",
        "    strRk = strRank.split('[')\n",
        "    index = 1\n",
        "\n",
        "    # assuming up to 5 promotions (no officer has more than 4)\n",
        "    for i in range(1, 5):\n",
        "        arr2.append(strRk[index][2:-1])\n",
        "        index += 1\n",
        "        arr2.append(strRk[index].replace(']', ''))\n",
        "        index += 1\n",
        "        if index >= len(strRk):\n",
        "            while i < 5:\n",
        "                arr2.append(0)\n",
        "                arr2.append(0)\n",
        "                i += 1\n",
        "            break\n",
        "   \n",
        "    while len(arr2) < 14:\n",
        "        arr2.append(0)\n",
        "    \n",
        "    # Extract wage history ( format e.g. [FY2018][$1,000] [FY2019][$1,000])\n",
        "    strWg = row[1][10]\n",
        "\n",
        "    # iterate through splits to extract data into array form\n",
        "    strWg = strWg.split('[')\n",
        "    index = 1\n",
        "    year = int(strWg[index][2:-1]) # last wage year\n",
        "\n",
        "    # remove wages for years past 2018 (reasoning in paper)\n",
        "    while year > 2018:\n",
        "        index += 2\n",
        "        if index > len(strRk):\n",
        "            break\n",
        "        year = int(strWg[index][2:-1])\n",
        "    \n",
        "    # Extract wages for 2000 to 2018 range (if NA/unemployed -> 0)\n",
        "    counter = 0\n",
        "    for i in range(2018, 1999, -1):\n",
        "    \n",
        "        if i == year:\n",
        "            index += 1\n",
        "            arr2.append(strWg[index][1:-1].replace(',', '').replace(']', '').replace(' ', ''))\n",
        "            counter += 1\n",
        "            index += 1\n",
        "            if index >= len(strWg):\n",
        "                continue\n",
        "            year = int(strWg[index][2:-1])\n",
        "        else:\n",
        "            arr2.append(0)\n",
        "            counter += 1\n",
        "  \n",
        "    # append to final data set\n",
        "    arr3.append(arr2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIUScUwr8Qvh"
      },
      "source": [
        "### Clean officer information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Z9e4gkeQlD0y"
      },
      "source": [
        "# Create new dataframe using extracted officer data & name columns\n",
        "newDf = pd.DataFrame(arr3, columns = ['Name', 'Rank', 'County', 'StartYear','rk1', 'rk1yr', 'rk2', 'rk2yr', 'rk3', 'rk3yr', 'rk4', 'rk4yr', \n",
        "                                 'rk5', 'rk5yr',  'wg18', 'wg17', 'wg16', 'wg15', 'wg14', 'wg13', 'wg12', 'wg11', 'wg10', 'wg09', 'wg08', 'wg07', \n",
        "                                   'wg06', 'wg05', 'wg04', 'wg03', 'wg02', 'wg01', 'wg00'])\n",
        "\n",
        "# Remove name duplicates due to complexity in matching (reasoning in paper)\n",
        "newDf.drop_duplicates(subset=['Name'], keep=False)\n",
        "\n",
        "# Use rank dictionary to number rank structure\n",
        "newDf = newDf.replace({\"Rank\": rank_dict})\n",
        "newDf = newDf.replace({\"rk1yr\": rank_dict})\n",
        "newDf = newDf.replace({\"rk2yr\": rank_dict})\n",
        "newDf = newDf.replace({\"rk3yr\": rank_dict})\n",
        "newDf = newDf.replace({\"rk4yr\": rank_dict})\n",
        "\n",
        "# Keep track of total\n",
        "newDf['CompNum'] = 0\n",
        "\n",
        "# Rank throughout the years\n",
        "for year in years:\n",
        "    newDf['rk' + year] = 0\n",
        "\n",
        "# Promotions throughout the years\n",
        "for year in years:\n",
        "    newDf['pr' + year] = 0\n",
        "\n",
        "# Extract data into sparse matrix and fill out rank & promotions for every year\n",
        "# Promotion is defined as: change in year to year rank seniority \n",
        "# (can be negative if demoted)\n",
        "\n",
        "for row in newDf.iterrows():\n",
        "    index = row[0]\n",
        "    rankYr =int(row[1][4])\n",
        "    rank = row[1][5]\n",
        "    year = 2018\n",
        "    while year >= rankYr:\n",
        "        if year < 2000:  break\n",
        "        newDf['rk' + str(year)][index] = rank\n",
        "        year -= 1\n",
        "    \n",
        "    prevRank = rankYr\n",
        "    rankYr = int(row[1][6])\n",
        "    rank = row[1][7]\n",
        "    if rankYr == 0:\n",
        "        continue\n",
        "    newDf['pr' + str(prevRank)][index] = row[1][5] - rank # change in ranks\n",
        "    \n",
        "    while year >= rankYr:\n",
        "        if year < 2000:  break\n",
        "        newDf['rk' + str(year)][index] = rank\n",
        "        year -= 1\n",
        "    \n",
        "    prevRank = rankYr\n",
        "    rankYr = int(row[1][8])\n",
        "    rank = row[1][9]\n",
        "    if rankYr == 0:\n",
        "        continue\n",
        "    newDf['pr' + str(prevRank)][index] = row[1][7] - rank\n",
        "    \n",
        "    while year >= rankYr:\n",
        "        if year < 2000:  break\n",
        "        newDf['rk' + str(year)][index] = rank\n",
        "        year -= 1\n",
        "      \n",
        "    prevRank = rankYr\n",
        "    rankYr = int(row[1][10])\n",
        "    rank = row[1][11]\n",
        "    if rankYr == 0:\n",
        "        continue\n",
        "    newDf['pr' + str(prevRank)][index] = row[1][9]- rank\n",
        "    while year >= rankYr:\n",
        "        if year < 2000:  break\n",
        "        newDf['rk' + str(year)][index] = rank\n",
        "        year -= 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P54lMqot7pd0"
      },
      "source": [
        "### Match Complaint Data to Officers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zwcRhLJlD0z"
      },
      "source": [
        "# Match Complaint Data to individual officers\n",
        "\n",
        "# Keep track of # of complaints matched\n",
        "found = 0\n",
        "notFound = 0\n",
        "\n",
        "# Number of complaints against officer in given year\n",
        "for year in years:\n",
        "    newDf[year] = 0\n",
        "\n",
        "# Number of FADO complaint types against officer in total\n",
        "for alleg in fadoAllegations:\n",
        "    newDf[alleg] = 0\n",
        "  \n",
        "# Number of results (e.g. exonerated) against officer in total\n",
        "for result in results:\n",
        "    newDf[result] = 0\n",
        "\n",
        "# Number of specific complaint types against officer in total\n",
        "for alleg in allegations:\n",
        "    newDf[alleg] = 0\n",
        "\n",
        "# For every complaint if name matches officer dataset extract data \n",
        "for row in copyDf.iterrows():\n",
        "    name = row[1][11]\n",
        "    indices = newDf[newDf['Name']== name].index.values # Name match\n",
        "\n",
        "    # Does name match officer data set?\n",
        "    if len(indices) > 0:\n",
        "        index = indices[0]\n",
        "        fado = row[1][6]\n",
        "        allegation = row[1][7]\n",
        "        result = row[1][8]\n",
        "        date = str(int(row[1][14]))\n",
        "        \n",
        "        # Insert information unless nan\n",
        "        if type(fado) != float:\n",
        "            newDf[fado][index] += 1\n",
        "        \n",
        "        if type(allegation) != float:\n",
        "            if((allegation != 'Force') and (allegation != 'Abuse of Authority')) and (allegation != 'Discourtesy'):\n",
        "                    newDf[allegation][index] += 1\n",
        "            else:\n",
        "                newDf['2'+allegation][index] += 1\n",
        "        if type(result) != float:\n",
        "            newDf[result][index] += 1\n",
        "            \n",
        "        newDf[date][index] += 1 # Total complaints in year + 1\n",
        "        newDf['CompNum'][index] += 1 # Total complaints + 1\n",
        "        found += 1\n",
        "        \n",
        "    else:\n",
        "        notFound += 1\n",
        "\n",
        "# Print number found\n",
        "print(found)\n",
        "print(notFound)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNk6fJ1L8d-Y"
      },
      "source": [
        "###  Export Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlEX1CTllD00"
      },
      "source": [
        "# Export New Data Set \n",
        "newDf.to_csv('cleanData.csv')\n",
        "\n",
        "# Make a copy of newDf and drop non-numeric values\n",
        "xNew = newDf.copy()\n",
        "xNew = xNew.drop(['Name'], axis=1)\n",
        "xNew = xNew.drop(['County'], axis=1)\n",
        "xNew = xNew.apply(pd.to_numeric) #transform to numeric\n",
        "\n",
        "# OPTIONAL: Export CSV file of only numeric entries\n",
        "# xNew.to_csv('cleanDataNew.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFFLGqnY9FCE"
      },
      "source": [
        "# NetworkX Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dncMIcF0EQ6R"
      },
      "source": [
        "### Create Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwxSjxnh5nZ4"
      },
      "source": [
        "# Create Network Graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Sot by complaint Id  to find partners in same complaint\n",
        "df = df.sort_values(by=\"Complaint Id\")\n",
        "\n",
        "# Iterate through complaints and identify partners\n",
        "# Add edges between partners\n",
        "prev = 0\n",
        "partners = []\n",
        "for index, row in df.iterrows():\n",
        "  currPerson  = row[\"Unique Id\"] # Unique Id\n",
        "  curr = row['Complaint Id'] # Complaint Id\n",
        "  G.add_node(currPerson)\n",
        "  if  curr == prev:\n",
        "    if currPerson not in partners:\n",
        "      for person in partners:\n",
        "        G.add_edge(person, currPerson) # update weights  \n",
        "  else:\n",
        "     partners = []\n",
        "  \n",
        "  if currPerson not in partners:\n",
        "    partners.append(currPerson)\n",
        "  prev = row['Complaint Id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVQ1cfhXlD17"
      },
      "source": [
        "# KModes\n",
        "km = KModes(n_clusters=4, init='Huang', n_init=5, verbose=1)\n",
        "dfCopy2['Command'] =dfCopy2['Command'].fillna('1')\n",
        "dfCopy2 = dfCopy2.fillna('1')\n",
        "clusters = km.fit_predict(dfCopy2)\n",
        "\n",
        "# Print the cluster centroids\n",
        "print(km.cluster_centroids_)\n",
        "print(kp.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x03e2fwHkNS"
      },
      "source": [
        "# Heatmap of Graph\n",
        "\n",
        "plt.subplot(121)\n",
        "print(heatmap(G, cbar= False, title ='MMSBM Simulation'))\n",
        "nbunch = [16205, 56121, 39959]\n",
        "nx.draw(G.subgraph(nbunch))\n",
        "# nx.draw(G)\n",
        "'''\n",
        "preds = nx.jaccard_coefficient(G, [(16205, 56121), (39959, 72558)])\n",
        "for u, v, p in preds:\n",
        "  print('(%d, %d) -> %.8f' % (u, v, p))\n",
        "'''\n",
        "\n",
        "# print(preds)\n",
        "print(sorted(G.degree, key=lambda x: x[1], reverse=True))\n",
        "# print(nx.clustering(G))\n",
        "# plt.subplot(122)\n",
        "\n",
        "nx.draw_random(G)\n",
        "plt.subplot(222)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKsSRa6lvp2f"
      },
      "source": [
        "### Add degree info to new data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0xmrj6HDbUB"
      },
      "source": [
        "Sort Graph by ID and match officers with their outgoing edge degrees & add findings to our new data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfDjSGYnlD05"
      },
      "source": [
        "# sort by degree\n",
        "Gsorted = sorted(G.degree, key=lambda x: x[1], reverse=True)\n",
        "print(Gsorted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "oQawBNlDlD05"
      },
      "source": [
        "graphdf = DataFrame(Gsorted,columns=['Unique Id','Degree'])\n",
        "print(graphdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs04g6IWlD06"
      },
      "source": [
        "graphdf['Name']=0\n",
        "for index in range(0,len(graphdf)):\n",
        "    indices = df.index[df['Unique Id'] == graphdf['Unique Id'][index]].tolist()\n",
        "    graphdf['Name'][index]=df['Full Name'][indices[0]] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bt0CtJLtlD1s"
      },
      "source": [
        "# Add degree to newDf\n",
        "newDf['degree'] = 0\n",
        "for index in range(0,len(graphdf)):\n",
        "    name = graphdf['Name'][index]\n",
        "    degree = graphdf['Degree'][index]\n",
        "    indices = newDf[newDf['Name']== name].index.values\n",
        "    if len(indices) > 0:\n",
        "        newDf['degree'][indices[0]] = degree\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "xE1jbOhElD1t"
      },
      "source": [
        "# Create binary variable \"bad apple\" for officers \n",
        "# with more than 4 outgoing edges\n",
        "\n",
        "newDf['badApple'] = 0\n",
        "for row in newDf.iterrows():\n",
        "    index = row[0]\n",
        "    degree = row[1][228]\n",
        "    if degree <= 4: newDf['badApple'][index] = 0\n",
        "    else: newDf['badApple'][index] = 1\n",
        "\n",
        "# Optional: Value Counts\n",
        "# newDf['degree'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17MLa-q3EHi0"
      },
      "source": [
        "### Predict Bad Apples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMeqiTMfEnq4"
      },
      "source": [
        "#### Linear Regression (cont.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nirCDBqalD1t"
      },
      "source": [
        "# Remove any leakage and non-numeric values\n",
        "# Perform Feature Selection\n",
        "xNew = newDf.drop(['degree'], axis=1)\n",
        "xNew = xNew.drop(['badApple'], axis=1)\n",
        "xNew = xNew.drop(['County'], axis=1)\n",
        "xNew = xNew.drop(['Name'], axis=1)\n",
        "\n",
        "years = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18']\n",
        "\n",
        "# Drop allegations (feature selection)\n",
        "for alg in allegations:\n",
        "    xNew = xNew.drop([alg], axis=1)\n",
        "    # Optional Remove wages:\n",
        "    # xNew = xNew.drop(['wg' + year], axis=1)\n",
        "\n",
        "# Split in train, test sets\n",
        "xPred = newDf['degree']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    xNew, xPred, test_size=0.2, random_state=42)\n",
        "\n",
        "linreg = LogisticRegression()\n",
        "lin_params = [{'fit_intercept': [True, False]}]\n",
        "\n",
        "# Perform Grid Search\n",
        "grid = GridSearchCV(linreg,\n",
        "                         param_grid=lin_params,\n",
        "                         scoring='neg_mean_squared_error', #sklearn optimizing by maximizing negative MSE\n",
        "                         n_jobs=1,\n",
        "                         verbose=2,\n",
        "                         cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "prediction_df = grid.predict(X_test)\n",
        "\n",
        "regr = linear_model.LinearRegression()\n",
        "regr.fit(X_train, y_train)\n",
        "prediction_df = regr.predict(X_test)\n",
        "# The coefficients\n",
        "# print('Coefficients: \\n', grid.coef_)\n",
        "# The mean squared error\n",
        "print('Mean squared error: %.2f'\n",
        "      % mean_squared_error(y_test, prediction_df))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print('Coefficient of determination: %.2f'\n",
        "      % r2_score(y_test, prediction_df))\n",
        "\n",
        "columns = xNew.columns\n",
        "\n",
        "# Print Coefficients with column name\n",
        "for i in range(0, len(columns)):\n",
        "    print(columns[i] + \" \" + str(regr.coef_[i]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoPBIBShFLTK"
      },
      "source": [
        "#### Additional Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "lYmL69vClD1u"
      },
      "source": [
        "# Perform additional Feature selection and test on NB\n",
        "\n",
        "clf = ExtraTreesClassifier(n_estimators=10)\n",
        "clf = clf.fit(xNew, xPred)\n",
        "print(clf.feature_importances_)\n",
        "\n",
        "model = SelectFromModel(clf, prefit=True)\n",
        "X_new = model.transform(xNew) \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_new, xPred, test_size=0.2, random_state=42)\n",
        "\n",
        "# Choose Features here\n",
        "\n",
        "# Perform BNB and test \n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "bnbScore = bnb.score(X_test, y_test)\n",
        "preds = bnb.predict(X_train)\n",
        "print(np.unique(preds, return_counts=True))\n",
        "print(bnbScore)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVpskxLkFx_n"
      },
      "source": [
        "#### Logistic Regression (binary)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "n4bLjGKIlD1v"
      },
      "source": [
        "# Grid Search on Logistic Regression\n",
        "parameters = {'solver':('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')}\n",
        "lr = GridSearchCV(LogisticRegression(), parameters)\n",
        "lr.fit(X_train, y_train)\n",
        "estimator = lr.best_estimator_\n",
        "print(estimator)\n",
        "clf = estimator\n",
        "clf.predict(X_test)\n",
        "clfscore = clf.score(X_test, y_test)\n",
        "print(clfscore)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKI7ylAqF6sY"
      },
      "source": [
        "#### Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prxUvgJwlD1v"
      },
      "source": [
        "# Note: this portion is taken from our HW2 submission\n",
        "def make_model(activation_function, num_hidden_layers, hidden_layer_size): \n",
        "  model = Sequential()\n",
        "\n",
        "    # Single layer model\n",
        "  if num_hidden_layers == 0: # then just specify a single layer, 1 is size of output\n",
        "        model.add(Dense(1, \n",
        "                        input_dim=X_train.shape[1], \n",
        "                        activation=activation_function,\n",
        "                        use_bias=True))\n",
        "        model.add(Dropout(0.5))\n",
        "    \n",
        "    # Specify initial layer with a hidden layer\n",
        "  if num_hidden_layers >= 1: \n",
        "        model.add(Dense(hidden_layer_size, \n",
        "                        input_dim=X_train.shape[1], \n",
        "                        activation=activation_function,\n",
        "                        use_bias=True))\n",
        "        model.add(Dropout(0.5))\n",
        "    \n",
        "    # Now add additional hidden layers\n",
        "  for i in range(0,num_hidden_layers-1):\n",
        "        model.add(Dense(hidden_layer_size, \n",
        "                        activation=activation_function, \n",
        "                        use_bias=True))\n",
        "        model.add(Dropout(0.5))\n",
        "    \n",
        "  if num_hidden_layers > 0:       \n",
        "        model.add(Dense(1)) # Final output layer, don't add if no hidden layers\n",
        "\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                  optimizer='adam')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJVjwuyPlD10"
      },
      "source": [
        "classifier = KerasRegressor(make_model, batch_size=32, epochs=1)\n",
        "\n",
        "params = [{'num_hidden_layers': [0],\n",
        "          'hidden_layer_size': [0],\n",
        "          'activation_function': ['linear', 'sigmoid', 'relu', 'tanh']},\n",
        "          {'num_hidden_layers': [1,2,3],\n",
        "          'hidden_layer_size': [64, 128, 256],\n",
        "          'activation_function': ['linear', 'sigmoid', 'relu', 'tanh']}]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rp7vQKRlD10"
      },
      "source": [
        "xNew = newDf.drop(['degree'], axis=1)\n",
        "xNew = xNew.drop(['badApple'], axis=1)\n",
        "xNew = xNew.drop(['County'], axis=1)\n",
        "xNew = xNew.drop(['Name'], axis=1)\n",
        "\n",
        "# Perform Grid Search on neural network\n",
        "# WARNING: MIght overwhelm and break Kernel!\n",
        "X_train, X_test, y_train, y_test = train_test_split(xNew, newDf['badApple'], test_size=0.20, random_state=12345)\n",
        "grid = GridSearchCV(classifier,\n",
        "                         param_grid=params,\n",
        "                         scoring='neg_mean_squared_error', #sklearn optimizing by maximizing negative MSE\n",
        "                         n_jobs=1,\n",
        "                         verbose=2,\n",
        "                         cv=5,# Number of folds for CV\n",
        "                     \n",
        "                   )\n",
        "\n",
        "# Fit Model\n",
        "X_train = np.asarray(X_train).astype('float32')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "print('The parameters of the best model are: ')\n",
        "print(grid.best_params_)\n",
        "\n",
        "\n",
        "best_model = grid.best_estimator_ #scikit-wrapped best model\n",
        "preds = best_model.predict(X_test)\n",
        "clfscore = bets_model.score(X_test, y_test)\n",
        "print(clfscore)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BKnsAlRUlD11"
      },
      "source": [
        "# Neural Network predictiveness(93.9%)\n",
        "preds = clf.predict(X_test)\n",
        "clfscore = clf.score(X_test, y_test)\n",
        "print(clfscore)\n",
        "print(np.unique(preds, return_counts=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMv1E89alD11"
      },
      "source": [
        "# Evaluate Paramters of best model \n",
        "print('The parameters of the best model are: ')\n",
        "print(grid.best_params_)\n",
        "\n",
        "best_model = grid.best_estimator_ #scikit-wrapped best model\n",
        "\n",
        "# Additional evaluation metrics\n",
        "X_test = np.asarray(X_test).astype('float32')\n",
        "preds = best_model.predict(X_test)\n",
        "preds=np.rint(preds)\n",
        "print(preds)\n",
        "rmse=np.sqrt(np.mean((y_test-preds)**2))\n",
        "clfscore = best_model.score(X_test, y_test)\n",
        "print(rmse)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5EmpbbuGiTH"
      },
      "source": [
        "#### Lasso Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EJfymIrHlD12"
      },
      "source": [
        "# Train model using CV\n",
        "X_train, X_test, y_train, y_test = train_test_split(xNew[['Force', 'Offensive Language', 'Discourtesy', 'Abuse of Authority']], newDf['badApple'], test_size=0.20, random_state=12345)\n",
        "lasso = LassoCV().fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "importance = np.abs(lasso.coef_)\n",
        "feature_names = np.array(X_train.columns)\n",
        "plt.bar(height=importance, x=feature_names)\n",
        "plt.title(\"Feature importances via coefficients\")\n",
        "plt.show()\n",
        "\n",
        "# Detailed feature importances\n",
        "for i in range(0, len(importance)):\n",
        "    print(str(importance[i]) + ' ' + feature_names[i])\n",
        "    \n",
        "\n",
        "# Lasso Evaluation\n",
        "print(lasso.score(X_test, y_test))\n",
        "\n",
        "X_test = np.asarray(X_test).astype('float32')\n",
        "preds = lasso.predict(X_test)\n",
        "preds=np.rint(preds)\n",
        "print(preds)\n",
        "\n",
        "# Evaluate false pos/neg\n",
        "i = 0\n",
        "falsePos = 0\n",
        "falseNeg = 0\n",
        "correct = 0\n",
        "for y in y_test:\n",
        "\n",
        "    if preds[i] == y:\n",
        "        if y == 1:\n",
        "        correct += 1\n",
        "    elif preds[i] > y:\n",
        "        falsePos += 1\n",
        "    else:\n",
        "        falseNeg += 1\n",
        "    i += 1\n",
        "    \n",
        "\n",
        "# Print Findings\n",
        "print(correct/i)\n",
        "print(falsePos/i)\n",
        "print(falseNeg/i)\n",
        "\n",
        "rmse=np.sqrt(np.mean((y_test-preds)**2))\n",
        "clfscore = lasso.score(X_test, y_test)\n",
        "print(rmse)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK3zaHzJJ5Sy"
      },
      "source": [
        "### Subgraph Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "J3QwaEw4lD2A"
      },
      "source": [
        "# See percentage of complaints caused by certain subgropus of NetworkX graph \n",
        "file1 = r\"subgraph.csv\"\n",
        "subgraph = pd.read_csv(file1)\n",
        "\n",
        "\n",
        "# ACTION: Choose to be analyzed sugraph; comment out one of the ids!\n",
        "\n",
        "# 22k officers in largest subgraph of NetworkX graph\n",
        "ids = subgraph['Unique ID']\n",
        "\n",
        "# Worst 22 instigators from NetworkX graph\n",
        "ids = [56121, 39959, 16205,48065, 49153, 48223,80598, 38157, 55704, 14854, 54730, 48343, 38305, 38148, 54534, 43801,15858, 55188, 19736, 72537,\n",
        "       55991, 48086]\n",
        "\n",
        "\n",
        "# Initially, sum all complaint IDs where officers of ids array are involved \n",
        "counter = 0\n",
        "no = 0\n",
        "complaints = []\n",
        "for row in df.iterrows():\n",
        " \n",
        "    if row[1][0] in ids:\n",
        "        # if len(row) < 5: continue\n",
        "        if math.isnan(row[1][5]):\n",
        "            counter += 1\n",
        "        else:\n",
        "            complaints.append(int(row[1][5]))\n",
        "\n",
        "# Next, sum up total number of complaints caused by given complaint ids\n",
        "for comp in df['Complaint Id']:\n",
        "    if math.isnan(comp): continue\n",
        "    if int(comp) in complaints:\n",
        "        counter += 1\n",
        "    else:\n",
        "        no += 1\n",
        "\n",
        "# Print out percentage of total complaints subgraph is responsible for\n",
        "print(counter / (counter + no))\n",
        "print(no)    \n",
        "\n",
        "# Interesting findings can be found in paper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTLapPNXm-Oo"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTp_6Z8ulD0n"
      },
      "source": [
        "# Seperate X and y\n",
        "copyDf = xNew.copy()\n",
        "xNew = xNew[xNew['wg15'] > 0] \n",
        "xPred = xNew['wg16'] # to be predicted\n",
        "xNew = xNew.drop(['wg16'], axis=1)\n",
        "xNew  xNew.drop(['wg17'], axis=1)\n",
        "xNew = xNew.drop(['wg18'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXVeuy1ElD0o"
      },
      "source": [
        "# Use Linear regression to predict wages in 2016 using past wages, complaints, rank, ...\n",
        "\n",
        "# Randomized train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    xNew, xPred, test_size=0.33, random_state=42)\n",
        "\n",
        "# Grid Search on LinReg\n",
        "linreg = LinearRegression(normalize = False)\n",
        "lin_params = [{'fit_intercept': [True, False], 'normalize': [True, False]}]\n",
        "grid = GridSearchCV(linreg,\n",
        "                         param_grid=lin_params,\n",
        "                         scoring='neg_mean_squared_error', #sklearn optimizing by maximizing negative MSE\n",
        "                         n_jobs=1,\n",
        "                         verbose=2,\n",
        "                         cv=5)\n",
        "\n",
        "# Choose which model to fit on:\n",
        "\n",
        "# 1. Grid Search\n",
        "grid.fit(X_train, y_train)\n",
        "prediction_df = grid.predict(X_test)\n",
        "\n",
        "# 2. Reg LinearRegression\n",
        "regr = linear_model.LinearRegression()\n",
        "regr.fit(X_train, y_train)\n",
        "prediction_df = regr.predict(X_test)\n",
        "\n",
        "# Evaluation: \n",
        "\n",
        "# The coefficients (printed later)\n",
        "# print('Coefficients: \\n', grid.coef_)\n",
        "# The mean squared error\n",
        "print('Mean squared error: %.2f'\n",
        "      % mean_squared_error(y_test, prediction_df))\n",
        "# The coefficient of determination: \n",
        "print('Coefficient of determination: %.2f'\n",
        "      % r2_score(y_test, prediction_df))\n",
        "\n",
        "# Print Coefficients with column names\n",
        "columns = xNew.columns\n",
        "for i in range(0, len(columns)):\n",
        "    print(columns[i] + \" \" + str(regr.coef_[i]))\n",
        "\n",
        "prediction_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrN-8h-kH_BD"
      },
      "source": [
        "# PCA & MCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEDfkP3TxZjl"
      },
      "source": [
        "## MCA on original data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WiSqyeEK_Sa"
      },
      "source": [
        "NOTE: This section analyzes the original complaint data set. No interesting findings found. For the optimized PCA, see the section on \"PCA on Improved Data Set\", which uses our own data set and is much more promising."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2klxrfthH7vx"
      },
      "source": [
        "#Fit MCA on complaint data set \n",
        "mca_df = mca.MCA(df[, cols=None][, ncols=None][, benzecri=True][, TOL=1e-4])\n",
        "principalComponents = pca.fit_transform(df)\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV-U9izLlD19"
      },
      "source": [
        "# Print resulting matrix & table\n",
        "mca_ben = mca.MCA(df)\n",
        "mca_ind = mca.MCA(df, benzecri=False)\n",
        "data = {'I': pd.Series(mca_ind.L),\n",
        "        'I': mca_ind.expl_var(greenacre=False, N=4),\n",
        "        'Z': pd.Series(mca_ben.L),\n",
        "        'Z': mca_ben.expl_var(greenacre=False, N=4),\n",
        "        'c': pd.Series(mca_ben.L),\n",
        "        'c': mca_ind.expl_var(greenacre=True, N=4)}\n",
        "\n",
        "# 'Indicator Matrix', 'Benzecri Correction', 'Greenacre Correction'\n",
        "columns = ['I', 'I', 'Z', 'Z', 'c', 'c']\n",
        "table2 = pd.DataFrame(data=data, columns=columns).fillna(0)\n",
        "table2.index += 1\n",
        "table2.loc[''] = table2.sum()\n",
        "table2.index.name = 'Factor'\n",
        "\n",
        "# Print table\n",
        "table2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzbU5pYhlD1-"
      },
      "source": [
        "# Fit MCA solely on Rank, Command, & Allegation\n",
        "dfCopy = df[['Rank', 'Command', 'Allegation']]\n",
        "mca.fit(dfCopy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E48PNBfhlD1-"
      },
      "source": [
        "# Plot coordinates - 1\n",
        "mca.plot_coordinates(\n",
        "     X=dfCopy,\n",
        "     ax=None,\n",
        "     figsize=(6, 6),\n",
        "     show_row_points=True,\n",
        "     row_points_size=10,\n",
        "     show_row_labels=False,\n",
        "     show_column_points=True,\n",
        "     column_points_size=30,\n",
        "     show_column_labels=False,\n",
        "     legend_n_cols=1\n",
        " )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_39_W21lD1-"
      },
      "source": [
        "# Plot coordinates - 2\n",
        "mca.plot_coordinates(dfCopy,\n",
        "                     row_points_alpha=.2,\n",
        "                     figsize=(10, 10),\n",
        "                     show_column_labels=True\n",
        "                    );"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9Hr_f7nlD1_"
      },
      "source": [
        "# Show MCA Grayscale Figure\n",
        "points = table3.loc[fs].values\n",
        "labels = table3.columns.values\n",
        "\n",
        "plt.figure()\n",
        "plt.margins(0.1)\n",
        "plt.axhline(0, color='gray')\n",
        "plt.axvline(0, color='gray')\n",
        "plt.xlabel('Factor 1')\n",
        "plt.ylabel('Factor 2')\n",
        "plt.scatter(*points, s=120, marker='o', c='r', alpha=.5, linewidths=0)\n",
        "for label, x, y in zip(labels, *points):\n",
        "    plt.annotate(label, xy=(x, y), xytext=(x + .03, y + .03))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAqpLa1mlD2A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xTYGl_ZnPhl"
      },
      "source": [
        "## PCA on Improved Data Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SypjOMwBNi35"
      },
      "source": [
        "### Initial PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z1ySlUSnUvc"
      },
      "source": [
        "# Read in our cleaned data set (attached to paper in Zip file)\n",
        "file1 = r\"/content/drive/Shareddrives/COS424/hw3/cleanData.csv\" # our own improved data set (can be exported above)\n",
        "df = pd.read_csv(file1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvSC2h1DnWpT"
      },
      "source": [
        "# Remove non-numeric olumns\n",
        "del df['Name']\n",
        "del df['County']\n",
        "\n",
        "# Add two new columns\n",
        "df['Complaint Code'] = 0 # Number of complaints in categorical form (0 - lowest // 3 - highest)\n",
        "df['Guilty'] = 0 # Number of times found guilty (0 - lowest // 3 - highest)\n",
        "\n",
        "# Fill categorical columns\n",
        "counter = 0\n",
        "for row in df.iterrows():\n",
        "    index = row[1][0]\n",
        "    if row[1][32]==0: df['Complaint Code'][index] = 0\n",
        "    elif row[1][32]<5: df['Complaint Code'][index] = 1\n",
        "    elif row[1][32]<20: df['Complaint Code'][index] = 2\n",
        "    else: df['Complaint Code'][index] = 3\n",
        "\n",
        "for row in df.iterrows():\n",
        "    index = row[1][0]\n",
        "    if row[1][69]==0: df['Guilty'][index] = 0\n",
        "    elif row[1][69]<5: df['Guilty'][index] = 1\n",
        "    elif row[1][69]<20: df['Guilty'][index] = 2\n",
        "    else: df['Guilty'][index] = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwOWZweHnfGT"
      },
      "source": [
        "x = dfCopy.values\n",
        "# Standardizing the features\n",
        "x = StandardScaler().fit_transform(x)\n",
        "\n",
        "# PCA with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "principalComponents = pca.fit_transform(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87HvVr2lnhJ9"
      },
      "source": [
        "# Print explained variance and ratio\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2'])\n",
        "print(pca.explained_variance_)\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33SyRrpZnm3A"
      },
      "source": [
        "# Print figure using components\n",
        "fig = plt.figure()\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "ax.scatter(principalDf['principal component 1'],principalDf['principal component 2'])\n",
        "\n",
        "# Print plt figure\n",
        "plt.scatter(principalDf['principal component 1'],principalDf['principal component 2'], alpha=.1, color='black')\n",
        "plt.xlabel('PCA 1')\n",
        "plt.ylabel('PCA 2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J00KuvJBnrel"
      },
      "source": [
        "# Add rank to principalDf with 2 components\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2'])\n",
        "\n",
        "finalDf = pd.concat([principalDf, df['Rank']], axis = 1)\n",
        "finalDf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJI1mQ7nntNQ"
      },
      "source": [
        "# KMeans Inertia Analysis\n",
        "ks = range(1, 10)\n",
        "inertias = []\n",
        "for k in ks:\n",
        "    # Create a KMeans instance with k clusters: model\n",
        "    model = KMeans(n_clusters=k)\n",
        "    \n",
        "    # Fit model to samples\n",
        "    model.fit(principalComponents)\n",
        "    \n",
        "    # Append the inertia to the list of inertias\n",
        "    inertias.append(model.inertia_)\n",
        "    \n",
        "# Plot inertia findings (in paper)\n",
        "plt.plot(ks, inertias, '-o', color='black')\n",
        "plt.xlabel('number of clusters, k')\n",
        "plt.ylabel('inertia')\n",
        "plt.xticks(ks)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OksCuOefnwrB"
      },
      "source": [
        "# Evaluate KMeans Performance and print findings\n",
        "kmeans_pca=KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
        "kmeans_pca.fit(principalComponents)\n",
        "cluster = kmeans_pca.labels_\n",
        "print(cluster)\n",
        "clusterDF = pd.DataFrame(cluster)\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2'])\n",
        "\n",
        "finalDf = pd.concat([principalDf, clusterDF], axis = 1)\n",
        "print(finalDf[finalDf.columns[2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Mu5dqznyN1"
      },
      "source": [
        "# This section is inspired from:\n",
        "#https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
        "\n",
        "# Print PCA in visualy pleasing way\n",
        "# Note: this section can be adjusted to fit different targets.\n",
        "\n",
        "fig = plt.figure(figsize = (8,8)) \n",
        "ax = plt.axes()\n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('3 component PCA', fontsize = 20)\n",
        "targets = [1, 2,3,4]\n",
        "colors = ['c','m','r','b']\n",
        "for target, color in zip(targets,colors):\n",
        "    indicesToKeep = finalDf[finalDf.columns[2]] == target\n",
        "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = 1)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0ct4LTkn_l8"
      },
      "source": [
        "pca1 = pca.components_[0]\n",
        "pca2 = pca.components_[1]\n",
        "\n",
        "# Help on this section came from:\n",
        "#https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
        "ind1 = np.argpartition(pca1, -5)[-5:]\n",
        "ind2 = np.argpartition(pca2, -5)[-5:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIsNeTZ5NnY6"
      },
      "source": [
        "### Additional 2D PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EDyujdKNMUI"
      },
      "source": [
        "Note: The remainder this section is a repeat of the previous section with an additionl column and different targets. There is a strong overlap in code and thus commenting is kept to a minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mS19b_yoGoT"
      },
      "source": [
        "# Create new complaints column (make continuous into categorical)\n",
        "# 0 - lowest // 3 - highest number of compliants\n",
        "df['Complaints']=0\n",
        "comp = 0\n",
        "for row in df.iterrows():\n",
        "    index = row[1][0]\n",
        "    if row[1][32]==0: df['Complaints'][index] = 0\n",
        "    elif row[1][32]<5: df['Complaints'][index] = 1\n",
        "    elif row[1][32]<20: df['Complaints'][index] = 2\n",
        "    else: df['Complaints'][index] = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJZzVNf7oORk"
      },
      "source": [
        "# Perform PCA again with new column added\n",
        "x = dfRank.values\n",
        "# Standardizing the features\n",
        "x = StandardScaler().fit_transform(x)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(x)\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHuxNyycoRbj"
      },
      "source": [
        "# Same as above section with new \n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2'])\n",
        "\n",
        "finalDf = pd.concat([principalDf, df['Complaints']], axis = 1)\n",
        "finalDf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIAZSsPWoTR4"
      },
      "source": [
        "fig = plt.figure(figsize = (8,8)) \n",
        "ax = plt.axes()\n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = [0,1,2,\n",
        "           3]\n",
        "colors = ['c','m','r','g']\n",
        "for target, color in zip(targets,colors):\n",
        "    indicesToKeep = finalDf['Complaints'] == target\n",
        "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
        "        \n",
        "               , c = color\n",
        "               , s = 1)\n",
        "ax.legend(targets)\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7Pe3159oUG3"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "ks = range(1, 10)\n",
        "inertias = []\n",
        "for k in ks:\n",
        "    # Create a KMeans instance with k clusters: model\n",
        "    model = KMeans(n_clusters=k)\n",
        "    \n",
        "    # Fit model to samples\n",
        "    model.fit(principalComponents)\n",
        "    \n",
        "    # Append the inertia to the list of inertias\n",
        "    inertias.append(model.inertia_)\n",
        "    \n",
        "plt.plot(ks, inertias, '-o', color='black')\n",
        "plt.xlabel('number of clusters, k')\n",
        "plt.ylabel('inertia')\n",
        "plt.xticks(ks)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6V2Sv7roV8W"
      },
      "source": [
        "kmeans_pca=KMeans(n_clusters=2, init='k-means++', random_state=42)\n",
        "kmeans_pca.fit(principalComponents)\n",
        "cluster = kmeans_pca.labels_\n",
        "print(cluster)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPREhqJ4vbaL"
      },
      "source": [
        "# Additional Non-Essential Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yHa02UMwdVB"
      },
      "source": [
        "## Basic Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_peACeuOwoOi"
      },
      "source": [
        "The following section is left uncommented as none of its findings are relevant to our paper. As it was included in the starter notebook and helped us be familiarized with the data, however, we feel obliged to keep it in our notebook for transparency purposes. It was not coded by us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Aoa1YXGB7wc"
      },
      "source": [
        "officer_counts = df['Full Name'].value_counts().reset_index()\n",
        "officer_counts.columns = ['Officer', 'Number of complaints']\n",
        "officer_counts.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kby8_XgbP_xe"
      },
      "source": [
        "fig, ax = plt.subplots(1, 3)\n",
        "fig.set_size_inches(18, 5)\n",
        "\n",
        "hist = officer_counts.hist(bins=100, range = [0, 20], ax=ax[0])\n",
        "hist = officer_counts.hist(bins=100, range = [20, 50], ax=ax[1])\n",
        "hist = officer_counts.hist(bins=100, range = [50, 200], ax=ax[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjOZGjppKzvG"
      },
      "source": [
        "allegations = df['Allegation'].value_counts().reset_index()\n",
        "allegations.columns = ['Allegation', 'Count of allegation']\n",
        "allegations.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJHbs3h7QHha"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2)\n",
        "fig.set_size_inches(15, 5)\n",
        "\n",
        "years_of_complaints = df.groupby([\"year\"]).size().reset_index(name=\"Number of Complaints per Year\").sort_values(by=['year'], ascending = True)\n",
        "\n",
        "years_of_complaints.plot(kind='bar',x='year',y='Number of Complaints per Year', ax=ax[0])\n",
        "\n",
        "months_of_complaints = df.groupby([\"month\"]).size().reset_index(name=\"Number of Complaints per Month\").sort_values(by=['month'], ascending = True)\n",
        "months_of_complaints.plot(kind='bar',x='month',y='Number of Complaints per Month', ax=ax[1])\n",
        "\n",
        "years = df['year'].unique()\n",
        "years = [x for x in years if (math.isnan(x) == False)]\n",
        "years.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVyWP1Mb6oXk"
      },
      "source": [
        "## Example use of Matrix Factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASM11QKQxOdo"
      },
      "source": [
        "The following section is left uncommented as none of its findings are relevant to our paper. As it was included in the starter notebook and helped us be familiarized with the data, however, we feel obliged to keep it in our notebook for transparency purposes. It was not coded by us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg9OhRuvx19c"
      },
      "source": [
        "officers    = np.array(df['Full Name']).astype(str)\n",
        "allegations = np.array(df['Allegation']).astype(str)\n",
        "\n",
        "# Convert from officer names and allegation names to label encoding.\n",
        "le1 = preprocessing.LabelEncoder()\n",
        "le1.fit(officers)\n",
        "\n",
        "le2 = preprocessing.LabelEncoder()\n",
        "le2.fit(allegations)\n",
        "\n",
        "xs = le1.transform(officers)\n",
        "ys = le2.transform(allegations)\n",
        "\n",
        "assert len(xs) == len(ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6s7bfnVzmSx"
      },
      "source": [
        "# Get counts of each officer/allegation interaction.\n",
        "non_unique_edges = zip(xs, ys)\n",
        "unique_edges_with_counts = pd.Series(non_unique_edges).value_counts()\n",
        "\n",
        "edges  = unique_edges_with_counts.index.values\n",
        "counts = unique_edges_with_counts.values\n",
        "\n",
        "u_xs, u_ys = zip(*edges)\n",
        "\n",
        "# Construct sparse matrix data type.\n",
        "X = csr_matrix((counts, (u_xs, u_ys)))\n",
        "\n",
        "# Compute Non-negative Matrix Factorization.\n",
        "n_components = 5\n",
        "nmf = NMF(n_components=n_components, random_state=41)\n",
        "nmf.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxGjVdgVrFnY"
      },
      "source": [
        "print(f'Top {n_components} components lead to reconstruction error (in Frobenius norm) of {nmf.reconstruction_err_}.')\n",
        "print()\n",
        "\n",
        "topK = 5\n",
        "for i in range(nmf.components_.shape[0]):\n",
        "  print(f'Top allegations in component {i+1}:')\n",
        "  allegs = np.argsort(nmf.components_[i])[::-1][:topK]\n",
        "  for j in range(topK):\n",
        "    print(f'\\t{le2.classes_[allegs[j]]}')\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpY-yUXY0MTj"
      },
      "source": [
        "# Compute truncated SVD (uses a randomized algorithm by default).\n",
        "n_components = 5\n",
        "svd = TruncatedSVD(n_components=n_components, random_state=41)\n",
        "svd.fit(X)\n",
        "\n",
        "# Look at the top principle components.\n",
        "print(f'Top {n_components} components explain {svd.explained_variance_ratio_.sum()} of the variance.')\n",
        "print(f'Breakdown: {svd.explained_variance_ratio_}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jqXvo8HFGFz"
      },
      "source": [
        "## CapStat.NYC Police Officer Database\n",
        "Compiled from CapStat.NYC by Wendy Ho\n",
        "\n",
        "https://github.com/wendyho/NYPD-Misconduct-Complaint-Database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1axrFzuFMLq"
      },
      "source": [
        "df3_nameseries = df3['Full Name'].values.astype(str)\n",
        "df_nameseries  = df['Full Name'].values.astype(str)\n",
        "\n",
        "df_match = df.loc[df['Full Name'].isin(df3_nameseries)]\n",
        "df3_match = df3.loc[df3['Full Name'].isin(df_nameseries)]\n",
        "print(f'{len(df_match)} of the {len(df)} complaints matched to a police officer in the CapStat.NYC database.')\n",
        "print(f'{len(df3_match)} of the {len(df3)} police officers have at least one complaint against them.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsvaD-KXIoOD"
      },
      "source": [
        "df3['Rank'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCYjg2Q2ItxO"
      },
      "source": [
        "df3['Location'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kr7EvT0aX1Y"
      },
      "source": [
        "#---------------------------- Run Time Params --------------------------------#\n",
        "\n",
        "# Probably going to try and use - flag conventions with __init__(self, *args, **kwargs)\n",
        "\n",
        "\n",
        "#---------------------------- Load Data --------------------------------------#\n",
        "num_people = 25\n",
        "num_groups = 5\n",
        "alpha = np.ones(num_groups).ravel()*0.1\n",
        "#B = np.eye(num_groups)*0.85\n",
        "#B = B + np.random.random(size=[num_groups,num_groups])*0.1\n",
        "\n",
        "B = np.eye(num_groups)*0.8\n",
        "B = B + np.ones([num_groups,num_groups])*0.2-np.eye(num_groups)*0.2\n",
        "\n",
        "#---------------------------- Setup Model -----------------------------------#\n",
        "raw_model = create_model(nmf.components_, num_people, num_groups, alpha, B)\n",
        "#model_instance = pymc.Model(raw_model)\n",
        "\n",
        "#---------------------------- Call MAP to initialize MCMC -------------------#\n",
        "#pymc.MAP(model_instance).fit(method='fmin_powell')\n",
        "print('---------- Finished Running MAP to Set MCMC Initial Values ----------')\n",
        "#---------------------------- Run MCMC --------------------------------------#\n",
        "print('--------------------------- Starting MCMC ---------------------------')\n",
        "M = pymc.MCMC(raw_model)\n",
        "M.sample(100000,50000, thin=5, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGjWCuRTsmfj"
      },
      "source": [
        "file4 = r\"/content/drive/Shareddrives/COS424/hw3/capstat_expanded_dataset.csv\" \n",
        "df4 = pd.read_csv(file4)\n",
        "df4.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQEwMw-P08z_"
      },
      "source": [
        "# Declaring Model\n",
        "model = KMeans(n_clusters=3)\n",
        "\n",
        "# Fitting Model\n",
        "model.fit(nmf.components_)\n",
        "\n",
        "# Predicitng a single input\n",
        "all_predictions = model.predict(nmf.components_)\n",
        "\n",
        "# Printing Predictions\n",
        "print(all_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN37nB-G2z3K"
      },
      "source": [
        "\n",
        "#filter rows of original data\n",
        "filtered_label0 = df[label == 0]\n",
        " \n",
        "#plotting the results\n",
        "plt.scatter(filtered_label0[:,0] , filtered_label0[:,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y67qWzwi3qLG"
      },
      "source": [
        "#Getting the Centroids\n",
        "centroids = model.cluster_centers_\n",
        "u_labels = np.unique(all_predictions)\n",
        " \n",
        "#plotting the results:\n",
        " \n",
        "for i in u_labels:\n",
        "    plt.scatter(df[all_predictions == i , 0] , df[all_predictions == i , 1] , \n",
        "                all_predictions = i)\n",
        "plt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}